{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Pedro\n",
      "[nltk_data]     Alves\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Pedro\n",
      "[nltk_data]     Alves\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cat_prod_links(url):\n",
    "    response = requests.get(first_page_url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    product_links = []\n",
    "\n",
    "    # Step 2: Navigate to the specific elements using the provided indentations\n",
    "    main_content = soup.find(\"main\", id=\"mainContent\", class_=\"responsiveProductListPage_mainContent responsiveProductListPage_mainContent_withFacets\")\n",
    "\n",
    "    if main_content:\n",
    "        # Step 3: Navigate to the next level (div with class 'productListProducts')\n",
    "        product_list_div = main_content.find(\"div\", class_=\"productListProducts\")\n",
    "        if product_list_div:\n",
    "            # Step 4: Navigate to the next level (ul with class 'productListProducts_products')\n",
    "            product_list_ul = product_list_div.find(\"ul\", class_=\"productListProducts_products\")\n",
    "            if product_list_ul:\n",
    "                # Step 5: Navigate to each product (li with class 'productListProducts_product')\n",
    "                products = product_list_ul.find_all(\"li\", class_=\"productListProducts_product\")\n",
    "                for product in products:\n",
    "                    # Step 6: Navigate to the next level (div with class 'athenaProductBlock')\n",
    "                    athena_product_block_div = product.find(\"div\", class_=\"athenaProductBlock\")\n",
    "                    if athena_product_block_div:\n",
    "                        # Step 7: Extract the link (a with class 'athenaProductBlock_linkImage')\n",
    "                        link_a = athena_product_block_div.find(\"a\", class_=\"athenaProductBlock_linkImage\")\n",
    "                        if link_a and 'href' in link_a.attrs:\n",
    "                            product_link = url + link_a['href']\n",
    "                            product_links.append(product_link)\n",
    "\n",
    "    return product_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_links = get_cat_prod_links(\"https://www.myprotein.pt/nutrition/protein/protein-isolate.list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paginated_reviews(reviews_url):\n",
    "\n",
    "    response = requests.get(reviews_url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # first page reviews\n",
    "    reviews = []\n",
    "    for review in soup.find_all(class_=\"athenaProductReviews_review\"):\n",
    "        review_text = review.find(class_='athenaProductReviews_reviewContent').text\n",
    "        review_stars = review.find(class_=\"athenaProductReviews_reviewRatingStarsContainer\")['aria-label']\n",
    "        reviews.append((review_text, review_stars, reviews_url))\n",
    "\n",
    "    # paginated reviews\n",
    "    try:\n",
    "        next_page_url = soup.find(\"a\", class_=\"athenaProductReviews_paginationNav athenaProductReviews_paginationNav-next\")[\"href\"]\n",
    "    except TypeError:\n",
    "        next_page_url = False\n",
    "\n",
    "    if next_page_url:\n",
    "        while next_page_url:\n",
    "            response = requests.get(next_page_url)\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "            for review in soup.find_all(class_=\"athenaProductReviews_review\"):\n",
    "                review_text = review.find(class_='athenaProductReviews_reviewContent').text\n",
    "                review_stars = review.find(class_=\"athenaProductReviews_reviewRatingStarsContainer\")['aria-label']\n",
    "                reviews.append((review_text, review_stars, next_page_url))\n",
    "\n",
    "            try:\n",
    "                next_page_url = soup.find(\"a\", class_=\"athenaProductReviews_paginationNav athenaProductReviews_paginationNav-next\")[\"href\"]\n",
    "            except TypeError:\n",
    "                break \n",
    "\n",
    "    return reviews\n",
    "    \n",
    "\n",
    "def get_first_page_reviews(soup, url):\n",
    "    reviews = []\n",
    "    # Step 2: Navigate to the specific elements using the provided indentations\n",
    "    main_content = soup.find(\"main\", id=\"mainContent\", class_=\"athenaProductPage\")\n",
    "    if main_content:\n",
    "        review_div = soup.find(\"div\", class_=\"athenaProductPage_productReviews\")\n",
    "        if review_div:\n",
    "            prod_review_div = review_div.find(\"div\", class_=\"athenaProductReviews\")\n",
    "            if prod_review_div:\n",
    "                if prod_review_div.find(\"div\", class_=\"athenaProductReviews_empty\"):\n",
    "                    reviews.append((None, None, url))\n",
    "                elif prod_review_div.find(\"div\", class_=\"athenaProductReviews_summary\"):\n",
    "                    review_container = prod_review_div.find(\"div\", class_=\"athenaProductReviews_summary_reviewContainer\")\n",
    "                    if review_container:\n",
    "                        summ_cols = review_container.find(\"div\", class_=\"athenaProductReviews_summary-columns\")\n",
    "                        if summ_cols:\n",
    "                            summ_right = summ_cols.find(\"div\", class_=\"athenaProductReviews_summary-right\")\n",
    "                            if summ_right:\n",
    "                                top_review_div = summ_right.find(\"div\", class_=\"athenaProductReviews_topReviews\")\n",
    "                                if top_review_div:\n",
    "                                    review_containers = top_review_div.find_all(\"div\", class_=\"athenaProductReviews_topReviewSingle\")\n",
    "                                    # Loop through each review container and extract the text\n",
    "                                    for review_container in review_containers:\n",
    "                                        review_text = review_container.find(\"p\", class_=\"athenaProductReviews_topReviewsExcerpt\").get_text(strip=True) if review_container.find(\"p\", class_=\"athenaProductReviews_topReviewsExcerpt\") else None\n",
    "                                        review_stars = review_container.find(\"div\", class_=\"athenaProductReviews_topReviewsRatingStarsContainer\")['aria-label']\n",
    "                                        reviews.append((review_text, review_stars, url))\n",
    "    else:\n",
    "        reviews.append((None, None, url))\n",
    "    \n",
    "    return reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews(product_links):\n",
    "\n",
    "    reviews = []\n",
    "    reviews_to_return = []\n",
    "    for prod_link in product_links:\n",
    "        response = requests.get(prod_link)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # navigate to reviews page\n",
    "        try:\n",
    "            reviews_url = soup.find(class_=\"athenaProductReviews_seeReviewsButton\")['href']\n",
    "        except TypeError:\n",
    "            reviews_url = False\n",
    "\n",
    "        if reviews_url:\n",
    "            reviews = get_paginated_reviews(reviews_url)\n",
    "        elif reviews_url == False:\n",
    "            reviews = get_first_page_reviews(soup, prod_link)\n",
    "        else:\n",
    "            reviews.append((None, None, prod_link))\n",
    "        \n",
    "        reviews_to_return += reviews \n",
    "\n",
    "    return reviews_to_return\n",
    "\n",
    "total_reviews = get_reviews(product_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text, stars, url = zip(*total_reviews)\n",
    "df = pd.DataFrame({'stars': stars, 'review': review_text, 'url': url})\n",
    "# select only number of stars\n",
    "df['stars'] = df['stars'].str.split(\" \").str[0]\n",
    "# remove odd whitespaces\n",
    "df['review'] = df['review'].str.strip().str.replace(r'\\s+', ' ', regex=True).str.replace(\"\\n\",\"\")\n",
    "df['product_name'] = df['url'].str.split(\"/\").str[-2]\n",
    "df['review'].fillna('No review available', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['processed_review'] = df['review'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_score(text):\n",
    "    if isinstance(text, str):\n",
    "        sentiment = analyzer.polarity_scores(text)\n",
    "        return sentiment\n",
    "    else:\n",
    "        # Handle non-string (float) cases, return np.nan\n",
    "        return {'neg': None, 'neu': None, 'pos': None, 'compound': None}\n",
    "\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "sentiment_scores = df['processed_review'].apply(get_sentiment_score)\n",
    "df[['Negative', 'Neutral', 'Positive', 'Compound']] = pd.DataFrame(sentiment_scores.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Compound</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>clear-whey-isolada-conjunto-de-saquetas-de-amostra</th>\n",
       "      <td>0.524000</td>\n",
       "      <td>0.476000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.296000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clear-whey-isolate</th>\n",
       "      <td>0.011556</td>\n",
       "      <td>0.983500</td>\n",
       "      <td>0.004944</td>\n",
       "      <td>0.018522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>impact-whey-isolate</th>\n",
       "      <td>0.018994</td>\n",
       "      <td>0.936101</td>\n",
       "      <td>0.044937</td>\n",
       "      <td>0.099458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>impact-whey-isolate-amostra</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>myprotein-clear-whey-isolate-sample</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>myprotein-impact-native-whey-isolate-sample</th>\n",
       "      <td>0.524000</td>\n",
       "      <td>0.476000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.296000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>myvegan-proteina-de-ervilha-isolada</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pack-inicial-clear-whey</th>\n",
       "      <td>0.524000</td>\n",
       "      <td>0.476000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.296000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>proteina-de-ervilha-isolada</th>\n",
       "      <td>0.012468</td>\n",
       "      <td>0.967435</td>\n",
       "      <td>0.020081</td>\n",
       "      <td>0.028460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>proteina-isolada-de-soja</th>\n",
       "      <td>0.016985</td>\n",
       "      <td>0.958500</td>\n",
       "      <td>0.024515</td>\n",
       "      <td>0.056804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>proteina-isolada-de-soja-amostra</th>\n",
       "      <td>0.524000</td>\n",
       "      <td>0.476000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.296000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Negative   Neutral  \\\n",
       "product_name                                                             \n",
       "clear-whey-isolada-conjunto-de-saquetas-de-amostra  0.524000  0.476000   \n",
       "clear-whey-isolate                                  0.011556  0.983500   \n",
       "impact-whey-isolate                                 0.018994  0.936101   \n",
       "impact-whey-isolate-amostra                         0.000000  1.000000   \n",
       "myprotein-clear-whey-isolate-sample                 0.000000  1.000000   \n",
       "myprotein-impact-native-whey-isolate-sample         0.524000  0.476000   \n",
       "myvegan-proteina-de-ervilha-isolada                 0.000000  1.000000   \n",
       "pack-inicial-clear-whey                             0.524000  0.476000   \n",
       "proteina-de-ervilha-isolada                         0.012468  0.967435   \n",
       "proteina-isolada-de-soja                            0.016985  0.958500   \n",
       "proteina-isolada-de-soja-amostra                    0.524000  0.476000   \n",
       "\n",
       "                                                    Positive  Compound  \n",
       "product_name                                                            \n",
       "clear-whey-isolada-conjunto-de-saquetas-de-amostra  0.000000 -0.296000  \n",
       "clear-whey-isolate                                  0.004944  0.018522  \n",
       "impact-whey-isolate                                 0.044937  0.099458  \n",
       "impact-whey-isolate-amostra                         0.000000  0.000000  \n",
       "myprotein-clear-whey-isolate-sample                 0.000000  0.000000  \n",
       "myprotein-impact-native-whey-isolate-sample         0.000000 -0.296000  \n",
       "myvegan-proteina-de-ervilha-isolada                 0.000000  0.000000  \n",
       "pack-inicial-clear-whey                             0.000000 -0.296000  \n",
       "proteina-de-ervilha-isolada                         0.020081  0.028460  \n",
       "proteina-isolada-de-soja                            0.024515  0.056804  \n",
       "proteina-isolada-de-soja-amostra                    0.000000 -0.296000  "
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(by=\"product_name\")[['Negative', 'Neutral', 'Positive', 'Compound']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
